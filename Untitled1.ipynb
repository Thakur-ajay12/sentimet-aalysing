{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"\"\"We now look at how to extract some statistics the corpus such  the number of sentences etc. using tokenization. These statistics can later be used to  some parameters  training a model. Tokenization  the process by which big quantities of text are divided into smaller parts called tokens. It is crucial to understand the pattern in the text in order to perform various NLP tasks. These tokens are very useful  finding such patterns. NLTK has a very important module tokenize which further  of sub-modules\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We now look at how to extract some statistics the corpus such  the number of sentences etc.',\n",
       " 'using tokenization.',\n",
       " 'These statistics can later be used to  some parameters  training a model.',\n",
       " 'Tokenization  the process by which big quantities of text are divided into smaller parts called tokens.',\n",
       " 'It is crucial to understand the pattern in the text in order to perform various NLP tasks.',\n",
       " 'These tokens are very useful  finding such patterns.',\n",
       " 'NLTK has a very important module tokenize which further  of sub-modules']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(words))\n",
    "#words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing stopword(the word like is,a,etc that are not much useful we remove them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "st_w = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### as we see the full stop \" . \" is not in list so we can add it in list \n",
    "st_w.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = [] \n",
    "  \n",
    "for w in words: \n",
    "    if w not in st_w: \n",
    "        filtered_sentence.append(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmitizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can also use stamming but sometimes that dosent make any sence\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "look\n",
      "extract\n",
      "statistic\n",
      "corpus\n",
      "number\n",
      "sentence\n",
      "etc\n",
      "using\n",
      "tokenization\n",
      "These\n",
      "statistic\n",
      "later\n",
      "used\n",
      "parameter\n",
      "training\n",
      "model\n",
      "Tokenization\n",
      "process\n",
      "big\n",
      "quantity\n",
      "text\n",
      "divided\n",
      "smaller\n",
      "part\n",
      "called\n",
      "token\n",
      "It\n",
      "crucial\n",
      "understand\n",
      "pattern\n",
      "text\n",
      "order\n",
      "perform\n",
      "various\n",
      "NLP\n",
      "task\n",
      "These\n",
      "token\n",
      "useful\n",
      "finding\n",
      "pattern\n",
      "NLTK\n",
      "important\n",
      "module\n",
      "tokenize\n",
      "sub-modules\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in filtered_sentence:\n",
    "    print(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    \n",
    "#print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
